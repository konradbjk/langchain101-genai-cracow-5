{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI\n",
    "We will start with OpenAI model. I personally prefer using them via Azure, due to a different license. You can create a free Azure account and get a few credtis for start to play with GPT models.\n",
    "\n",
    "Please follow [Langchain documentation](https://python.langchain.com/v0.2/docs/integrations/chat/azure_chat_openai/) for more details about `AzureChatOpenAI` component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "gpt_4_turbo = AzureChatOpenAI(\n",
    "    api_version =os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "    temperature=0.4,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we will be doing now is chaning runnables. Read more about it on [Langchain docs](https://python.langchain.com/v0.2/docs/how_to/sequence/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | gpt_4_turbo\n",
    "chain.invoke(\"How many paws a dog has?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building RAG\n",
    "Over this project, we will be using Langchain components. You can read [their conceptual guide](https://python.langchain.com/v0.2/docs/concepts/) to learn more (if you missed my presentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document loading and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = './data/'\n",
    "\n",
    "with open(f\"{filepath}/Jak_zbudowaÄ‡_chatbot.txt\") as file:\n",
    "    webinar_1 = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "webinar_1_sentences = re.split(r'[.!?]+', webinar_1)\n",
    "len(webinar_1_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webinar_1_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader(filepath, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "text_docs = loader.load()\n",
    "len(text_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_filepath = f\"{filepath}Manual-Leadership-2024.pdf\"\n",
    "loader = PyPDFLoader(pdf_filepath)\n",
    "pdf_document = loader.load()\n",
    "len(pdf_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "images=convert_from_path(pdf_path=pdf_filepath)\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and now choose your OCR library and perform OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunking text into smaller pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def tiktoken_len(text):\n",
    "    \"\"\"calculating length of text in tokens not words/characters\"\"\"\n",
    "    tokens = tiktoken.encoding_for_model(\"gpt-4\").encode(\n",
    "        text,\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain created a Text Splitter playground, where you can try out various parameters. It is [hosted under streamlit](https://langchain-text-splitter.streamlit.app/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 300,\n",
    "    chunk_overlap = 30,\n",
    "    length_function = tiktoken_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(text_docs)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: *When processing multiple documents, worth to save your chunks (saves time)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "timestamp = datetime.timestamp(now)\n",
    "with open(f'langchain_documents-{timestamp}.jsonl', 'w', encoding=\"UTF-8\") as jsonl_file:\n",
    "    for doc in docs:\n",
    "        jsonl_file.write(doc.json() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store\n",
    "In this project, we are using Qdrant vector store. Please make sure to follow README instructions to set up a qdrant docker.\n",
    "\n",
    "You can learn more about Qdrant component on [Langchain documentation](https://python.langchain.com/v0.2/docs/integrations/vectorstores/qdrant/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_community.vectorstores import Qdrant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new qdrant collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"langchain-demo\"\n",
    "url=os.getenv(\"QDRANT_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantVectorStore.from_documents(\n",
    "    docs,\n",
    "    azure_embeddings,\n",
    "    url = url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also connect to an existing collection\n",
    "\n",
    "Qdrant < v1.10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "client = qdrant_client.QdrantClient(url=url)\n",
    "\n",
    "qdrant2 = Qdrant(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=azure_embeddings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For qdrant > 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant2 = QdrantVectorStore.from_existing_collection(collection_name, azure_embeddings, url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search\n",
    "Now let us run the most basic search over our vector stores. You can find more information about those queries on [Qdrant's Docummentation](https://qdrant.tech/documentation/frameworks/langchain/#similarity-search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which large language models (llm) are good?\"\n",
    "\n",
    "qdrant2.embeddings.embed_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = qdrant2.similarity_search_with_score(query=query,score_threshold=0.44, k=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Time to finally create our RAG application pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant2.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.44, \n",
    "        \"k\": 5\n",
    "        })\n",
    "\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "def retriever_with_score(query: str) -> List[Document]:\n",
    "    docs, scores = zip(*qdrant2.similarity_search_with_score(query))\n",
    "    for doc, score in zip(docs, scores):\n",
    "        doc.metadata[\"score\"] = score\n",
    "\n",
    "    return docs\n",
    "\n",
    "retriever_with_score.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "The questions you receive should be in regards to artifical intelligence and generative AI\\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "    \n",
    "{context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += f'<quote source={doc.metadata[\"source\"]}>{doc.page_content}</quote>\\n\\n'\n",
    "        \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "pipeline = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | gpt_4_turbo\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(pipeline.invoke(\"What is the difference between various LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiquery\n",
    "> The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. \n",
    "\n",
    "Source: [Langchain Docs](https://python.langchain.com/v0.2/docs/how_to/MultiQueryRetriever/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_retriever = MultiQueryRetriever.from_llm(retriever=retriever,llm=gpt_4_turbo)\n",
    "unique_docs = new_retriever.invoke(query)\n",
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_pipeline = (\n",
    "    {\n",
    "        \"context\": new_retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | gpt_4_turbo\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(multi_query_pipeline.invoke(\"What is the difference between various LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Bedrock (Claude Opus)\n",
    "During the presentation, we have been using Anthropic Claude 3 Opus model, via AWS Bedrock. You can learn more about AWS Bedrock <> Langchain integration it from the [Langchain Docs](https://python.langchain.com/v0.2/docs/integrations/chat/bedrock/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "claude_opus = ChatBedrock(\n",
    "    region_name=\"us-west-2\",\n",
    "    model_id=\"anthropic.claude-3-opus-20240229-v1:0\",\n",
    "    model_kwargs=dict(temperature=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_query_pipeline_opus = (\n",
    "    {\n",
    "        \"context\": new_retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | claude_opus\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(multi_query_pipeline.invoke(\"What is the difference between various LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavily Search\n",
    "This retriever, is documented in more details on the [Langchain docs](https://python.langchain.com/v0.2/docs/integrations/retrievers/tavily/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "\n",
    "tavily_retriever.invoke(\"What is the difference between various LLMs?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Retriever\n",
    "Time to connect information from two various retrievers and (basic) rerank the results. You can learn more about the insights of the Ensemble retriver from the [dedicated Langchain docs](https://python.langchain.com/v0.2/docs/how_to/ensemble_retriever/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[new_retriever, tavily_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "ensemble_retriever.invoke(\"What is the difference between various LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pipeline = (\n",
    "    {\n",
    "        \"context\": ensemble_retriever | format_docs, \n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | rag_prompt\n",
    "    | gpt_4_turbo\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "pprint(ensemble_pipeline.invoke(\"What is the difference between various LLMs?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
